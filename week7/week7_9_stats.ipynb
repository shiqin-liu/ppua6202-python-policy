{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: Inferential statistics and regression\n",
    "\n",
    "**In this module, we will learn about inferential statistics and regression modelling using python.**\n",
    "\n",
    "We will be using the following statistical Python packadges:\n",
    "\n",
    "1. [seaborn](http://seaborn.pydata.org/) for statistical visualization like scatter plot we learned earlier, such as frequency distribution, linear regression plot etc. These would be helpful for exploratory data visualization\n",
    "\n",
    "2. [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) for basic statistical testing (correlation, t-tests, etc.)\n",
    "\n",
    "3. [statsmodels](https://www.statsmodels.org/stable/examples/index.html) for more advanced statistical models, like Multiple regression and other complex models.\n",
    "\n",
    "**Key concepts**  \n",
    "Statistical inference is the process of using a sample to *infer* the characteristics of an underlying population (from which this sample was drawn) through estimation and hypothesis testing. Contrast this with descriptive statistics, which focus simply on describing the characteristics of the sample itself. Key concepts include: \n",
    "- parameter estimation and confidence intervals \n",
    "- normal distribution, sampling distribution and Central Limit Theorem\n",
    "- statistical hypothesis testing\n",
    "- statistical modelling (eg. t-test, regression)\n",
    "- *p*-values\n",
    "\n",
    "To conduct statistical inference, we rely on *statistical models*: sets of assumptions plus mathematical relationships between variables, producing a formal representation of some theory. We are essentially trying to explain the process underlying the generation of our data. \n",
    "\n",
    "Readings:\n",
    "- Urdan, Statistics in Plain English, ch. 8, 9, 13.\n",
    "- Rumsey, Statistics For Dummies, 2nd Edition (optional, if you have not taken any statistical course before, this would be a good entry-level look for understanding the key concepts), available as free access online from NEU library\n",
    "\n",
    "In this notebook, we will focus on the basics of specifying, estimating, and interpreting regression models using Python tools. The goal is to make you a knowledgeable consumer of studies that use regression, and know of the available tools that would help you conduct statistical analysis using regression yourself.\n",
    "\n",
    "There is lots more to cover in a course on regression that we must skip for today's quick overview (such as interactions, transforming variables, handling multicollinearity, handling outliers, conducting diagnostics, etc.) That's why there are entire courses dedicated to regression analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.tools.tools import add_constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and prep the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the example topic we used in week 4, income inequality in US Metropolitan Statistical Areas (MSAs). We will conduct some statistical modelling to examine the relationship between income inequlaity and some characteristics of MSAs.\n",
    "\n",
    "Related working papers and publications by Census:  \n",
    "Income Inequality: https://www.census.gov/topics/income-poverty/income-inequality.html  \n",
    "Income Inequality among Regions and Metropolitan Statistical Areas: 2005 to 2015: https://www.census.gov/library/working-papers/2017/demo/SESHD-WP2017-41.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../data/hhincome_msa_05_15.csv', dtype={'msacode':str})\n",
    "\n",
    "# data cleaning and processing \n",
    "# drop non useful column\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# drop columns with no data\n",
    "df = df.dropna()\n",
    "\n",
    "# change year data type to int\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "# let's only focus on one-year data as an example\n",
    "df = df[df['year'] == 2015]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive statistics and visualization (review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a variable\n",
    "var = 'hhinc_50prct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gini', 'hhinc_50prct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two data subsets\n",
    "# subset the dataframe into large and small metropolitan areas\n",
    "df_lar_msa = df[df['tot_pop'] > 1000000]\n",
    "group1 = df_lar_msa[var]\n",
    "\n",
    "df_sml_msa = df[df['tot_pop'] < 1000000]\n",
    "group2 = df_sml_msa[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the probability distributions of these two groups?\n",
    "ax = sns.distplot(group1.dropna(), label='group1-large-cities')\n",
    "ax = sns.distplot(group2.dropna(), label='group2-small-cities', ax=ax)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability distributions indicate the likelihood of an event or outcome. A density plot is a representation of the distribution of a numeric variable, indicating the probability or the likelihood of an event. The peaks of the density plot are at the locations where there is the highest concentration of points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hypothesis testing: difference in means and t-tests \n",
    "\n",
    "In the above plot, we see the probability distribution of median household income seem to be different between large and small cities. But is the difference between two groups statistically significant? Large cities tend to have more economic resources, employment opportunities and business activities than small cities. Given this theory, I want to hypothesis test it. My hypothesis is that median household income is higher for large cities than small cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group1.mean())\n",
    "print(group2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference in means\n",
    "diff = group1.mean() - group2.mean()\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average large cities' median household income is approx $10,197 higher than the average small cities'. But is it statistically significant? To determine this, we calculate the t-statistic and its p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the t-stat and its p-value\n",
    "t_statistic, p_value = stats.ttest_ind(group1, group2, equal_var=False, nan_policy='omit')\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the difference in means statistically significant?\n",
    "# My chosen confidence level is 95% (and thus my significance level is 0.05).\n",
    "alpha = 0.05 #significance level\n",
    "p_value < alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember my original hypothesis: \"median household income is higher for large cities than small cities.\" Let's express it formally in the parlance of statistical hypothesis testing:\n",
    "\n",
    "H0: median household income in large cities are not higher than small cities (null hypothesis)  \n",
    "H1: median household income in large cities are higher than small cities (alternative hypothesis)\n",
    "\n",
    "The two possible outcomes of a hypothesis test are 1) I reject the null hypothesis or 2) I cannot reject the null hypothesis.\n",
    "\n",
    "My p-value is less than the desired significance level, therefore I can reject the null hypothesis. You can only reject the null hypothesis if p is less than the significance level (which itself is an arbitrarily chosen probability threshold). Rejecting the null hypothesis does not mean that we've proven the alternative hypothesis, but rather just that it provides some evidence for this alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# what is the difference in mean poverty_rate in large vs small cities?\n",
    "# is it statistically significant?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Scatter plot and correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are interested to know why some areas are more unequal than others? what urban and regional factors contribute to inequalities in cities? Normally we will ask these questions first and make some assumptions based on existing studies or theories to guess the factors that drive inequality in cities, then look for relevant data for the study. But for this example, let's think using the data we have on hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what variables might be related to income inequality in cities?\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a scatter plot to picture a relationship\n",
    "# You interpret a scatterplot by looking for trends in the data as you go from left to right\n",
    "# use seaborn to scatter-plot two variables\n",
    "ax = sns.scatterplot(x=df['poverty_rate'], y=df['gini'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the data show an uphill pattern as you move from left to right, this indicates a positive relationship between X and Y. As the X-values increase (move right), the Y-values increase (move up) a certain amount.  \n",
    "- If the data show a downhill pattern as you move from left to right, this indicates a negative relationship between X and Y. As the X-values increase (move right) the Y-values decrease (move down) by a certain amount.  \n",
    "- If the data don't seem to resemble any kind of pattern (even a vague one), then no relationship exists between X and Y.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above section “Interpreting a scatterplot,” I say data that resembles an uphill line has a positive linear relationship and data that resembles a downhill line has a negative linear relationship. However, I didn't address the issue of whether or not the linear relationship was strong or weak. \n",
    "\n",
    "Can one statistic measure both the strength and direction of a linear relationship between two variables? Sure! Statisticians use the correlation coefficient to measure the strength and direction of the linear relationship between two or more numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables that you think may have some levels of relationship\n",
    "x = 'gini'\n",
    "y = 'poverty_rate'\n",
    "\n",
    "# correlation matrix\n",
    "# how well are predictors correlated with response... and with each other?\n",
    "correlations = df[[x, y]].corr()\n",
    "correlations.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rules of thumb:\n",
    "- Exactly −1: A perfect downhill (negative) linear relationship\n",
    "- −0.70: A strong downhill (negative) linear relationship\n",
    "- −0.50: A moderate downhill (negative) relationship\n",
    "- −0.30: A weak downhill (negative) linear relationship\n",
    "- 0: No linear relationship\n",
    "- +0.30: A weak uphill (positive) linear relationship\n",
    "- +0.50: A moderate uphill (positive) relationship\n",
    "- +0.70: A strong uphill (positive) linear relationship\n",
    "- Exactly +1: A perfect uphill (positive) linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Simple (bivariate) linear regression\n",
    "In the case of two numerical variables x and y, when at least a moderate correlation has been established through both the correlation and the scatterplot, you know they have some type of linear relationship.  \n",
    "\n",
    "Researchers often use that relationship to predict the (average) value of y for a given value of x using a straight line. Statisticians call this line the regression line. If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y. In other words, you predict (the average) y from x.\n",
    "\n",
    "Simple (aka bivariate) regression has just 2 variables: one is used to predict the other.\n",
    "  \n",
    "  - **Response** variable = what you are predicting (synonyms: dependent variable, outcome variable, regressand)\n",
    "  - **Predictor** variable = what you are using to predict (synonyms: independent variable, feature, covariate, regressor)\n",
    "  \n",
    "\n",
    "In this example, I want to predict gini income inequality as a function of poverty rate. Therefore, I **specify** my model as $y = \\beta_0 + \\beta_1 \\times x_1$ where $y$ represents a city's gini index and $x_1$ represents a city's poverty rate. $\\beta_0$ (the intercept, aka constant) and $\\beta_1$ (the coefficient on poverty) are the model parameters to be estimated. My chosen confidence level is 95% (and thus my significance level is 0.05). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response and predictor\n",
    "response = 'gini'\n",
    "predictor = 'poverty_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see the regression line visually\n",
    "ax = sns.regplot(data=df, x=predictor, y=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter full dataset to retain only these columns and only rows without nulls in these columns\n",
    "data = df[[response, predictor]].dropna()\n",
    "print(data.shape)\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = data[predictor]\n",
    "y = data[response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I interpret this regression results table?**\n",
    "\n",
    "The coefficient (**coef**) is the estimated relationship between my variables (the slope of the line). The *p* value allow us to determine whether each predictor variable is statistically significantly related to the response variable.\n",
    "\n",
    "The p values associated with the predictor variable are much smaller than .05, indicating that my predictor variables is a significant predictor of my response variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So how good is my model? What does the $R^2$ value tell me? This single predictor explains about 12% the variation of the response. \n",
    "\n",
    "\n",
    "To explain more (and predict better), we need more predictors in our model as there should be other factors that may help predict the income inequalities in cities.\n",
    "\n",
    "### 4d. Multiple regression\n",
    "\n",
    "OLS regression with multiple predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response and predictors\n",
    "response = 'gini'\n",
    "predictors = ['unemp_rate', 'poverty_rate']\n",
    "\n",
    "# filter full dataset to retain only these columns and only rows without nulls in these columns\n",
    "data = df[[response] +  predictors].dropna()\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = data[predictors]\n",
    "y = data[response]\n",
    "\n",
    "# estimate a linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add in more variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix containing predictors (drop nulls), and a response variable vector\n",
    "predictors = ['unemp_rate', 'poverty_rate', 'black_alonerate', 'hispanic_anyrate', 'asian_alonerate']\n",
    "X = df[predictors].dropna()\n",
    "y = df.loc[X.index][response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a linear regression model\n",
    "Xc = add_constant(X)\n",
    "model = sm.OLS(y, Xc)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I interpret this multiple regression results table?**\n",
    "\n",
    "Each coefficient represents the individual predictor's relationship with the response, while holding all the other predictors constant. \n",
    "\n",
    "When looking at the variables and their associated p-values, all variables, except hispanic, have a p-value that is less than 0.05. This may indicate that these variables (i.e. unemp_rate, poverty_rate, black_alonerate, asian_alonerate) are significantly associated with the gini index.   \n",
    "\n",
    "When looking at the variables and their associated coefficients, for example, the coefficient of the unemployment rate is negative (-0.171), that says that a city's unemployment rate is negatively associated with a city's income gini index.   \n",
    "\n",
    "To interpret my results based on the size of the coefficient, I would say for every one unit (or one percentage point) increase in a city's unemployment rate, the household income gini index in a city is likely to decrease by 0.171, while holding other factors (in the model) constant.\n",
    "\n",
    "To interpret my results in plain language, I would say a city with higher unemployment rate is likely to have lower income inequality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# pick another variable and see if you could intreprete the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try different sets of predictors and see if R-squared increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression modeling steps**:\n",
    "\n",
    "  1. think through the relevant theory and assumptions\n",
    "  2. specify a model based on theory\n",
    "  3. collect data and clean/prep it\n",
    "  4. estimate model parameters using the data\n",
    "  5. interpret and report the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we add in more and more predictors in a regression model, the model results may indicate there might be some numerical error, there could be many reasons, like endogeneity, multicollinearity, outliers etc. We may want to take a step back to think about the validity of the model and do some diagnostic test (like to following correlation tests) to examine what might cause the problem. This is a also a huge topic, and require a lot of understanding of your data, related theories, and experience. We are not going to cover these in this lecture, but these are things I would like to be alert about. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a step back to examine the correlation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "# how well are predictors correlated with response... and with each other?\n",
    "correlations = df[predictors + sorted([response])].corr()\n",
    "correlations.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual correlation matrix via seaborn heatmap\n",
    "# use vmin, vmax, center to set colorbar scale properly\n",
    "sns.set(style='white')\n",
    "ax = sns.heatmap(correlations, vmin=-1, vmax=1, center=0,\n",
    "                 cmap='coolwarm', square=True, linewidths=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking back\n",
    "  - Fundamentals of programming with Python and Jupyter notebook  \n",
    "  - The basic Python programming language and its uses  \n",
    "  - Cleaning, manipulating, and analyzing data with Python’s pandas library  \n",
    "  - Descriptive statistics and data visualization \n",
    "  - Querying APIs and scraping for public open data  \n",
    "  - Spatial analysis and mapping  \n",
    "  - Inferential statistics and regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next\n",
    "\n",
    "- Make your work presentable\n",
    "    - Rule A, Birmingham A, Zuniga C, Altintas I, Huang S-C, Knight R, et al. (2019) [Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007). PLoS Comput Biol 15(7)\n",
    "    - Open science: share your code and data and avoid point-and-click software. \n",
    "  \n",
    "- Assignment6: run through this notebook, and complete the exercise in \"now your turn\" cell. Then submit this notebook\n",
    "\n",
    "- Final project\n",
    "\n",
    "- Continue your own Python Journey\n",
    "\n",
    "- Enjoy the rest of the summer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppua6202)",
   "language": "python",
   "name": "ppua6202"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
